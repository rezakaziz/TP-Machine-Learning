{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "432402cd",
   "metadata": {},
   "source": [
    "# TP 01 : Régression linéaire et logistique binaire\n",
    "\n",
    "Binômes : \n",
    "- Binôme 1\n",
    "- Binôme 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c5f906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7402ac7e",
   "metadata": {},
   "source": [
    "## I. Réalisation des algorithmes\n",
    "\n",
    "Cette partie sert à améliorer la compréhension les algorithmes d'apprentissage automatique vus en cours en les implémentant à partir de zéro. \n",
    "Pour ce faire, on va utiliser la bibliothèque **numpy** qui est utile dans les calcules surtout matricielles.\n",
    "\n",
    "### I.1. Regression linéaire \n",
    "\n",
    "Premièrement, on va implémenter les fonctions nécéssaires pour la régression linéaire.\n",
    "\n",
    "#### I.1.1. Fonction de prédiction\n",
    "\n",
    "$$Z = zf(X, theta) = \\theta_0 + \\sum\\limits_{j=1}^{N} \\theta_j X_j = \\sum\\limits_{j=0}^{N} \\theta_j X_j | X_0 = 1 $$\n",
    "\n",
    "- X : une matrice de M lignes (échantillons) et (N + 1) colonnes (caractéristiques). Avant d'utiliser cette fonction, on doit augmenter la matrice originale avec une colonne pour $X_0 = 1$ si on veut utiliser $\\theta_0$\n",
    "- $\\theta$ : un vecteur des paramétres avec une taille (N + 1)\n",
    "- Z : un vecteur de taille M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9173ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculer la combinaison linéaire\n",
    "def zf(X, theta):\n",
    "    return \n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : array([1. , 1.7, 2.4])\n",
    "#---------------------------------------------------------------------\n",
    "X_t = np.array([[1., 100.], [1., 200.], [1., 300.]])\n",
    "Theta_t = np.array([0.3, 0.007])\n",
    "zf(X_t, Theta_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6988768",
   "metadata": {},
   "source": [
    "#### I.1.2. Fonction du coût\n",
    "\n",
    "La fonction du coût calcule l'erreur entre les valeurs estimées ($H$) en utilisant la fonction de prédiction $z$ précédente (H = Z(X, theta) et les valeurs réelles ($Y$).\n",
    "Ici, on va utiliser l'erreur quadratique moyenne (MSE: mean square error). \n",
    "\n",
    "$$J = MSE(Y, H) = \\frac{1}{2M} \\sum\\limits_{i=1}^{M} (Y - H)^2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958b1e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Définir la fonction du coût MSE\n",
    "def MSE(Y, H):\n",
    "    return None\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : 0.016666666666666673\n",
    "#---------------------------------------------------------------------\n",
    "H_t = np.array([1. , 1.7, 2.4])\n",
    "Y_t = np.array([1., 2., 2.5])\n",
    "MSE(Y_t, H_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e8c95c",
   "metadata": {},
   "source": [
    "Le gradient de cette fonction est calculé comme suit : \n",
    "\n",
    "$$\\frac{\\partial MSE}{\\partial \\theta_j} \n",
    "= \\frac{\\partial }{\\partial \\theta_j} \\frac{1}{2M} \\sum\\limits_{i=1}^{M} (Y^{(i)} - H^{(i)})^2 \n",
    "= \\frac{1}{2M} \\sum\\limits_{i=1}^{M} [\\frac{\\partial }{\\partial \\theta_j} (Y^{(i)} - H^{(i)})^2]\n",
    "= \\frac{1}{2M} \\sum\\limits_{i=1}^{M} [-2 (Y^{(i)} - H^{(i)}) \\frac{\\partial }{\\partial \\theta_j} H^{(i)}]\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial MSE}{\\partial \\theta_j}\n",
    "= \\frac{1}{M} \\sum\\limits_{i=1}^{M} [(H^{(i)} - Y^{(i)}) \\frac{\\partial }{\\partial \\theta_j} \\sum\\limits_{k=0}^{N} \\theta_k X_k^{(i)}]\n",
    "= \\frac{1}{M} \\sum\\limits_{i=1}^{M} [(H^{(i)} - Y^{(i)}) \\frac{\\partial }{\\partial \\theta_j} \\theta_j X_j^{(i)}]\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial MSE}{\\partial \\theta_j} = \\frac{1}{M} \\sum\\limits_{i=1}^{M} (H^{(i)} - Y^{(i)}) X_j^{(i)}$$\n",
    "\n",
    "- \n",
    "Où $X^{(i)}$ est l'échantillon $i$ et $Y^{(i)}$ est la prédiction. $X_0^{(i)} = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72e20b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Définir la fonction de gradient MSE\n",
    "# Elle doit retourner les gradients de tous les thétas\n",
    "def dMSE(X, Y, H):\n",
    "    return None\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : array([ -0.13333333, -30.        ])\n",
    "#---------------------------------------------------------------------\n",
    "X_t = np.array([[1., 100.], [1., 200.], [1., 300.]])\n",
    "H_t = np.array([1. , 1.7, 2.4])\n",
    "Y_t = np.array([1., 2., 2.5])\n",
    "dMSE(X_t, Y_t, H_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fa935b",
   "metadata": {},
   "source": [
    "### I.2. Régression logistique\n",
    "\n",
    "Ici, on va implémenter la régression logistique qui se base sur la combinaison linéaire de la régression linaire.\n",
    "\n",
    "#### I.2.1. Fonction de prédiction\n",
    "\n",
    "$$P(X, theta) = Sg(zf(X, theta)) $$\n",
    "\n",
    "$$Sg(Z) = \\frac{1}{1+e^{-Z}}$$\n",
    "\n",
    "- X : une matrice de M lignes (échantillons) et (N + 1) colonnes (caractéristiques). Avant d'utiliser cette fonction, on doit augmenter la matrice originale avec une colonne pour $X_0 = 1$ si on veut utiliser $\\theta_0$\n",
    "- $\\theta$ : un vecteur des paramétres avec une taille (N + 1)\n",
    "- Z : un vecteur "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fae1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO La fonction ségmoid\n",
    "# Elle doit fonctionner sur des scalaires ou des vecteurs de numpy\n",
    "def sg(Z): \n",
    "    return None\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : (0.5, array([0.26894142, 0.73105858]))\n",
    "#---------------------------------------------------------------------\n",
    "sg(0), sg(np.array([-1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bb3c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Prédiction de probabilité dans la régression logistique\n",
    "# Probabilité de la régression logistique\n",
    "def Prl(X, Theta): \n",
    "    return None\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : array([0.5621765 , 0.99477987, 0.02297737])\n",
    "#---------------------------------------------------------------------\n",
    "X_t = np.array([[1., 0], [1., 10.], [1., -8]])\n",
    "Theta_t = np.array([0.25, 0.5])\n",
    "\n",
    "Prl(X_t, Theta_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31162455",
   "metadata": {},
   "source": [
    "#### I.2.2. Fonction du coût\n",
    "\n",
    "La fonction du coût calcule l'erreur entre les valeurs estimées ($H$).\n",
    "Ici, on va utiliser l'entropie croisée binaire (BCE: Binary Cross Entropy Loss). \n",
    "\n",
    "$$BCE(Y^{(i)}, H^{(i)})\n",
    "= \\begin{cases}\n",
    "- \\log(H^{(i)}) & \\text{ si } Y^{(i)} = 1\\\\ \n",
    "- \\log(1 - H^{(i)})  & \\text{ si } Y^{(i)} = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "Puisque $Y^{(i)} \\in \\{0, 1\\}$, donc : \n",
    "\n",
    "$$ BCE(Y^{(i)}, H^{(i)}) = - Y^{(i)} \\log(H^{(i)}) - (1- Y^{(i)}) \\log(1 - H^{(i)}) $$\n",
    "\n",
    "$$J = BCE(Y, H) = \\frac{-1}{M} \\sum\\limits_{i=1}^{M} [Y^{(i)} \\log(H^{(i)}) + (1- Y^{(i)}) \\log(1 - H^{(i)})]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4894b39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : La fonction de cout BCE \n",
    "def BCE(Y, H): \n",
    "    return None \n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : 0.6181210324473608\n",
    "#---------------------------------------------------------------------\n",
    "Y_t = np.array([1., 0., 0., 1.])\n",
    "H_t = np.array([0.25, 0.25, 0.5, 0.9])\n",
    "BCE(Y_t, H_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49877f9a",
   "metadata": {},
   "source": [
    "Le gradient de cette fonction est calculé comme suit : \n",
    "\n",
    "$$\\frac{\\partial BCE}{\\partial \\theta_j} \n",
    "= \\frac{-1}{M} \\sum\\limits_{i=1}^{M} \\frac{\\partial}{\\partial \\theta_j} [Y^{(i)} \\log(H^{(i)}) + (1- Y^{(i)}) \\log(1 - H^{(i)})]\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial BCE}{\\partial \\theta_j} \n",
    "= \\frac{-1}{M} \\sum\\limits_{i=1}^{M} [ Y^{(i)} \\frac{\\partial}{\\partial \\theta_j} \\log(H^{(i)}) + (1- Y^{(i)}) \\frac{\\partial}{\\partial \\theta_j}\\log(1 - H^{(i)})]\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial BCE}{\\partial \\theta_j} \n",
    "= \\frac{-1}{M} \\sum\\limits_{i=1}^{M} [ Y^{(i)} \\frac{1}{H^{(i)}} \\frac{\\partial}{\\partial \\theta_j} H^{(i)} + (1- Y^{(i)}) \\frac{-1}{1-H^{(i)}} \\frac{\\partial}{\\partial \\theta_j} H^{(i)})]\n",
    "= \\frac{-1}{M} \\sum\\limits_{i=1}^{M} \\frac{Y^{(i)}-H^{(i)}}{H^{(i)}(1-H^{(i)})} \\frac{\\partial}{\\partial \\theta_j} H^{(i)}\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial H^{(i)}}{\\partial \\theta_j} \n",
    "= \\frac{\\partial \\sigma(Z^{(i)})}{\\partial Z^{(i)}} \\frac{\\partial Z^{(i)}}{\\partial \\theta_j} \n",
    "= [\\sigma(Z^{(i)}) (1-\\sigma(Z^{(i)}))]\\frac{\\partial}{\\partial \\theta_j} \\sum\\limits_{k=0}^{N} \\theta_k X_k^{(i)}  \n",
    "= H^{(i)} (1-H^{(i)})  X_j^{(i)}\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial BCE}{\\partial \\theta_j} \n",
    "= \\frac{-1}{M} \\sum\\limits_{i=1}^{M} \\frac{Y^{(i)}-H^{(i)}}{H^{(i)}(1-H^{(i)})} [H^{(i)} (1-H^{(i)}) X_j^{(i)}]\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial BCE}{\\partial \\theta_j} = \\frac{1}{M} \\sum\\limits_{i=1}^{M} (H^{(i)} - Y^{(i)}) X_j^{(i)}$$\n",
    "\n",
    "- \n",
    "Où $X^{(i)}$ est l'échantillon $i$ et $Y^{(i)}$ est la prédiction. $X_0^{(i)} = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3132a0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Définir la fonction de gradient BCE\n",
    "def dBCE(X, Y, H): \n",
    "    return None\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : array([0.08333333, 0.25      ])\n",
    "#---------------------------------------------------------------------\n",
    "X_t = np.array([[1., 5.], [1., 10.], [1., 8]])\n",
    "H_t = np.array([2.75, 5.25, 4.25])\n",
    "Y_t = np.array([3., 5., 4.5])\n",
    "dMSE(X_t, Y_t, H_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec8c611",
   "metadata": {},
   "source": [
    "### I.3. Déscente du gradient \n",
    "\n",
    "\n",
    "#### I.3.1. Fonction de convergence\n",
    "\n",
    "Ici, on veut implémenter une fonction qui décide l'arrêt de la déscente du gradient en se basant sur : \n",
    "- L'erreur actuelle $J$ et l'erreur précédente $J0$\n",
    "- Nombre des itérations max $IT$ et l'itération actuelle $it$\n",
    "\n",
    "Cette fonction renvoit **True** si :\n",
    "- on a atteint le nombre maximal des itérations\n",
    "- l'erreur a augmenté (si var=True) sinon on ne prend pas la variation de l'erreur en considération"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f9077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Définir la fonction d'arret de la descente du gradient\n",
    "def arreter(J0, J, it_max, it, var=True):\n",
    "    return None\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : (False, True, True, False)\n",
    "#---------------------------------------------------------------------\n",
    "arreter(0.5, 0.2, 100, 0), arreter(0.1, 0.2, 100, 5), arreter(0.5, 0.2, 100, 100), arreter(0.1, 0.2, 100, 5, var=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684a1fc0",
   "metadata": {},
   "source": [
    "#### I.3.2. Fonction de mise à jour des paramètres\n",
    "\n",
    "$$\\theta_i = \\theta_i - \\alpha \\frac{\\partial J}{\\partial \\theta_j}$$\n",
    "\n",
    "Cette fonction doit retourner les nouveaux thetats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2469287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Définir la fonction de la mise à jours des paramètres\n",
    "def majTheta(Theta, Gradient, alpha): \n",
    "    return None\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : array([0.30013333, 0.037     ])\n",
    "#---------------------------------------------------------------------\n",
    "Theta_t = np.array([0.3, 0.007])\n",
    "Gradient_t = np.array([ -0.13333333, -30.])\n",
    "alpha_t = 0.001\n",
    "majTheta(Theta_t, Gradient_t, alpha_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66ae276",
   "metadata": {},
   "source": [
    "#### I.3.3. Déscente du gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5efd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descente(X, Y, Theta, Hf=zf, Jf=MSE, dJf=dMSE, it_max=100, alpha=0.1, var=True): \n",
    "\n",
    "    H0 = Hf(X, Theta)\n",
    "    J0 = Jf(Y, H0)\n",
    "    it = 0\n",
    "    couts = []\n",
    "    \n",
    "    #pour ne pas modifier le contenu de Theta en entrée\n",
    "    Theta = Theta.copy()\n",
    "    \n",
    "    while True:\n",
    "        Theta_opt = Theta.copy()\n",
    "        couts.append(J0)\n",
    "    \n",
    "        Gradient = dJf(X, Y, H0)\n",
    "        Theta = majTheta(Theta, Gradient, alpha)\n",
    "        H = Hf(X, Theta)\n",
    "        J = Jf(Y, H)\n",
    "        \n",
    "        if arreter(J0, J, it_max, it, var=var):\n",
    "            break\n",
    "        \n",
    "        J0 = J\n",
    "        H0 = H\n",
    "        it += 1\n",
    "    \n",
    "    return Theta_opt, couts, J\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : (array([0.3  , 0.007]), [0.016666666666666653], 20.117448897777773)\n",
    "#---------------------------------------------------------------------\n",
    "X_t = np.array([[1., 100.], [1., 200.], [1., 300.]])\n",
    "Theta_t = np.array([0.3, 0.007])\n",
    "Y_t = np.array([1., 2., 2.5])\n",
    "IT_max = 5\n",
    "alpha_t = 0.001\n",
    "\n",
    "descente(X_t, Y_t, Theta_t, it_max=IT_max, alpha=alpha_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3202df34",
   "metadata": {},
   "source": [
    "### I.4. Regroupement et test de l'implémentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b6d1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import outils\n",
    "\n",
    "class Regression(object):\n",
    "    \n",
    "    def __init__(self, it_max=100, logistique=False, alpha=0.01, norm=True, const=True, var=True): \n",
    "        self.it_max = it_max\n",
    "        self.alpha = alpha\n",
    "        self.norm = norm\n",
    "        self.const = const\n",
    "        self.var = var\n",
    "        \n",
    "        if logistique :\n",
    "            self.Hf=Prl\n",
    "            self.Jf=BCE\n",
    "            self.dJf=dBCE\n",
    "        else:\n",
    "            self.Hf=zf\n",
    "            self.Jf=MSE\n",
    "            self.dJf=dMSE   \n",
    "    \n",
    "    def entrainer(self, X, Y): \n",
    "        X_pre, self.mean, self.std = outils.preparer(X, norm=self.norm, const=self.const)\n",
    "        Theta = outils.generer_aleatoire_1(X_pre.shape[1])\n",
    "        self.Theta, self.couts, self.dernier_cout = descente(X_pre, Y, Theta, \n",
    "                                                             Hf=self.Hf, Jf=self.Jf, dJf=self.dJf, \n",
    "                                                             it_max=self.it_max, alpha=self.alpha, var=self.var)\n",
    "        \n",
    "    # La prédiction\n",
    "    # si classes=False elle rend H sinon elle rend 0 ou 1 au cas de regression logistique\n",
    "    def predire(self, X, classes=False, seuil=0.5):\n",
    "        X_pre, self.mean, self.std = outils.preparer(X, norm=self.norm, const=self.const, mean=self.mean, std=self.std)\n",
    "        H = self.Hf(X_pre, self.Theta)\n",
    "        if classes and logistique:\n",
    "            return H >= seuil\n",
    "        return H\n",
    "\n",
    "#=====================================================================\n",
    "# TEST\n",
    "#=====================================================================\n",
    "\n",
    "reg_lin = Regression(norm=False)\n",
    "X_t = np.array([[5.], [10.], [8.]])\n",
    "Y_t = np.array([3., 5., 4.5])\n",
    "reg_lin.entrainer(X_t, Y_t)\n",
    "\n",
    "plt.scatter(X_t[:,0], Y_t, color=\"blue\")\n",
    "plt.plot(X_t[:,0], reg_lin.predire(X_t), color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82d96c4",
   "metadata": {},
   "source": [
    "## II. Application et analyse\n",
    "\n",
    "Cette partie sert à appliquer les algorithmes, modifier les paramètres et analyser les résultats\n",
    "\n",
    "\n",
    "### II.1. Régression linéaire "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd8cdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"Superficie\", \"Prix\"]\n",
    "houses = pd.read_csv(\"datasets/houses.csv\", names=header)\n",
    "houses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c2c336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "Xhouses = houses.iloc[:, :-1].values # Premières colonnes \n",
    "Yhouses = houses.iloc[:,-1].values # Dernière colonne \n",
    "Xhouses_train, Xhouses_test, Yhouses_train, Yhouses_test = train_test_split(Xhouses, Yhouses, test_size=0.2, random_state=0)  \n",
    "\n",
    "len(Xhouses_train), len(Xhouses_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a295d0",
   "metadata": {},
   "source": [
    "#### II.1.1 Taux d'apprentissage et convergence \n",
    "\n",
    "Ici, nous avons varié le taux d'apprentissage et afficher les courbes de la fonction objective par rapport à l'itération. \n",
    "\n",
    "**TODO : Analyser les résultats**\n",
    "- Que remarquez-vous ?\n",
    "- Donner une hypothèse\n",
    "- Essayer de justifier cette hypothèse par ce que vous avez vu en cours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0989edc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAR = False\n",
    "NORM = True\n",
    "ITMAX = 500\n",
    "\n",
    "reg_lin1 = Regression(it_max=ITMAX, norm=NORM, alpha=1., var=VAR)\n",
    "reg_lin01 = Regression(it_max=ITMAX, norm=NORM, alpha=0.1, var=VAR)\n",
    "reg_lin001 = Regression(it_max=ITMAX, norm=NORM, alpha=0.01, var=VAR)\n",
    "reg_lin0001 = Regression(it_max=ITMAX, norm=NORM, alpha=0.001, var=VAR)\n",
    "\n",
    "reg_lin1.entrainer(Xhouses_train, Yhouses_train)\n",
    "reg_lin01.entrainer(Xhouses_train, Yhouses_train)\n",
    "reg_lin001.entrainer(Xhouses_train, Yhouses_train)\n",
    "reg_lin0001.entrainer(Xhouses_train, Yhouses_train)\n",
    "\n",
    "\n",
    "plt.plot(reg_lin1.couts, label = \"alpha=1\")\n",
    "plt.plot(reg_lin01.couts, label = \"alpha=0.1\")\n",
    "plt.plot(reg_lin001.couts, label = \"alpha=0.01\")\n",
    "plt.plot(reg_lin0001.couts, label = \"alpha=0.001\")\n",
    "plt.legend()\n",
    "#plt.autoscale()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d71e90",
   "metadata": {},
   "source": [
    "#### II.1.2 Prediction \n",
    "\n",
    "**TODO : Analyser les résultats**\n",
    "- Que remarquez-vous ?\n",
    "- Est-ce que la régression linéaire peut prédire exactement les valeurs Y ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0949468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression  \n",
    "regressor1 = LinearRegression(normalize=True)  \n",
    "regressor1.fit(Xhouses_train, Yhouses_train) \n",
    "\n",
    "Yhouses_pred = regressor1.predict(Xhouses_test)  \n",
    "\n",
    "plt.scatter(Xhouses_train, Yhouses_train, color=\"blue\", label = \"entrainement\")\n",
    "plt.scatter(Xhouses_test, Yhouses_test, color=\"green\", label = \"test\")\n",
    "plt.plot(Xhouses_train, regressor1.predict(Xhouses_train), color=\"red\", label = \"line de prediction\")\n",
    "plt.scatter(Xhouses_test, Yhouses_pred, color=\"violet\", label = \"prediction\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e327ee9b",
   "metadata": {},
   "source": [
    "#### II.1.3 Prediction avec régression multinomiale\n",
    "\n",
    "Pour avoir des préductions sous formes des courbes non linéaires, nous avons générer des nouvelles caractéristiques : \n",
    "- $X^2$\n",
    "- $X^5$\n",
    "- $X^{10}$\n",
    "\n",
    "**TODO : Analyser les résultats**\n",
    "- Que remarquez-vous ?\n",
    "- Donner une hypothèse (en précisant si plus de données ou avoir des données complexes peut améliorer la performance)\n",
    "- Essayer de justifier cette hypothèse par ce que vous avez vu en cours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d037eda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "degres = [2, 5, 10]\n",
    "erreurs = [mean_squared_error(Yhouses_test, Yhouses_pred)]\n",
    "couleurs=[\"green\", \"magenta\", \"blue\"]\n",
    "\n",
    "# Trouver les indexes ordonnées \n",
    "idx = Xhouses_train[:,0].argsort()\n",
    "\n",
    "plt.scatter(Xhouses_train[idx], Yhouses_train[idx], color=\"cyan\", label = \"entrainement\")\n",
    "plt.scatter(Xhouses_test, Yhouses_test, color=\"black\", label = \"test\")\n",
    "plt.plot(Xhouses_train, regressor1.predict(Xhouses_train), color=\"red\", label = \"line degre=1\")\n",
    "\n",
    "for i in range(len(degres)):\n",
    "    poly = PolynomialFeatures(degree=degres[i], include_bias=False)\n",
    "    Xhouse_train_poly = poly.fit_transform(Xhouses_train)\n",
    "    regresseur = LinearRegression(normalize=True)  \n",
    "    regresseur.fit(Xhouse_train_poly, Yhouses_train)  \n",
    "    plt.plot(Xhouses_train[idx], regresseur.predict(Xhouse_train_poly)[idx], color=couleurs[i], label = \"line degre=\" + str(degres[i]))\n",
    "    Xhouse_test_poly = poly.fit_transform(Xhouses_test)\n",
    "    Yhouses_pred = regresseur.predict(Xhouse_test_poly) \n",
    "    erreurs.append(mean_squared_error(Yhouses_test, Yhouses_pred))\n",
    "\n",
    "print(erreurs)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f584a8",
   "metadata": {},
   "source": [
    "### II.2. Régression logistique binaire "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727a03f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = pd.read_csv(\"datasets/notes.csv\")\n",
    "\n",
    "# Extraction des features \n",
    "X_notes = notes.iloc[:, :-1].values # Premières colonnes \n",
    "\n",
    "Y_notes = notes.iloc[:,-1].values # Dernière colonne \n",
    "\n",
    "notes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dbee9d",
   "metadata": {},
   "source": [
    "#### II.2.1 Normalisation et convergence \n",
    "\n",
    "Ici, on veut tester l'effet de la normalisation sur la convergence du modèle.\n",
    "\n",
    "**TODO : Analyser les résultats**\n",
    "- Que remarquez-vous ?\n",
    "- Donner une hypothèse\n",
    "- Essayer de justifier cette hypothèse par ce que vous avez vu en cours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0f1220",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAR = False\n",
    "ALPHA = 0.01\n",
    "ITMAX = 200\n",
    "\n",
    "reg_log_nonorm = Regression(it_max=ITMAX, norm=False, alpha=ALPHA, var=VAR, logistique=True)\n",
    "reg_log_nonorm.entrainer(X_notes, Y_notes)\n",
    "reg_log_norm = Regression(it_max=ITMAX, norm=True, alpha=ALPHA, var=VAR, logistique=True)\n",
    "reg_log_norm.entrainer(X_notes, Y_notes)\n",
    "\n",
    "plt.plot(reg_log_nonorm.couts, label = \"sans normalisation\")\n",
    "plt.plot(reg_log_norm.couts, label = \"avec normalisation\")\n",
    "\n",
    "plt.legend()\n",
    "#plt.autoscale()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c33c82",
   "metadata": {},
   "source": [
    "#### II.2.2 Complexité du modèle et apprentissage\n",
    "\n",
    "On veut améliorer la séparation entre les deux classes : \"admis\" et \"non admis\". Pour ce faire, on a proposé d'ajouter des caractéristiques complexes : $X^3$ et $X^5$.\n",
    "\n",
    "**TODO : Analyser les résultats**\n",
    "- Que remarquez-vous ? (les nouveaux modèles ont-il été capables d'améliorer le modèle initial ?)\n",
    "- Donner une hypothèse (Quelle est la raison ?) \n",
    "- Essayer de justifier cette hypothèse par ce que vous avez vu en cours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f8aba2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from outils import courbe_decision\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "degres = [3, 5]\n",
    "couleurs=[\"red\", \"magenta\", \"blue\"]\n",
    "\n",
    "def dessiner2D(modele, ax, xrange, yrange, poly=None, colors=[\"red\"], label=\"\"):\n",
    "    XX = [[[xr, yr] for xr in xrange] for yr in yrange]\n",
    "    XX = np.array(XX).reshape(-1, 2)\n",
    "    #grid = [[modele.predict(np.array([[xr, yr]])) for xr in xrange] for yr in yrange]\n",
    "    #grid = np.array(grid).reshape(len(xrange), len(yrange))\n",
    "    if poly:\n",
    "        XX = poly.fit_transform(XX)\n",
    "    grid = modele.predict(XX).reshape(len(xrange), len(yrange))\n",
    "    cs = ax.contour(xrange, yrange, grid, colors=colors,linewidths=(0.5),linestyles=('--'),levels=None)\n",
    "    cs.collections[1].set_label(label)\n",
    "\n",
    "\n",
    "X_notes_train = X_notes[:80,:]\n",
    "X_notes_test = X_notes[80:,:]\n",
    "Y_notes_train = Y_notes[:80]\n",
    "Y_notes_test = Y_notes[80:]\n",
    "# Trouver les indexes ordonnées \n",
    "idx2 = X_notes_train[:,0].argsort()\n",
    "\n",
    "oui_train = Y_notes_train == 1\n",
    "oui_test = Y_notes_test == 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "#ax.scatter(X_notes_train[:,0], X_notes_train[:,1], c=Y_notes_train, lw=0, alpha=1.)\n",
    "#ax.scatter(X_notes_test[:,0], X_notes_test[:,1], c=Y_notes_test, marker=\"x\", lw=0, alpha=1.)\n",
    "ax.scatter(X_notes_train[oui_train, 0], X_notes_train[oui_train, 1], color=\"green\", marker=\"o\", label=\"Admis (entrainement)\")\n",
    "ax.scatter(X_notes_train[~oui_train, 0], X_notes_train[~oui_train, 1], color=\"green\", marker=\"x\", label=\"Non Admis (entrainement)\")\n",
    "\n",
    "ax.scatter(X_notes_test[oui_test, 0], X_notes_test[oui_test, 1], color=\"red\", marker=\"o\", label=\"Admis (test)\")\n",
    "ax.scatter(X_notes_test[~oui_test, 0], X_notes_test[~oui_test, 1], color=\"red\", marker=\"x\", label=\"Non Admis (test)\")\n",
    "\n",
    "reglog1 = LogisticRegression(penalty='none')\n",
    "reglog1.fit(X_notes_train, Y_notes_train)\n",
    "\n",
    "xrange = np.linspace(0, 20, 50)\n",
    "yrange = np.linspace(0, 20, 50)\n",
    "dessiner2D(reglog1, ax, xrange, yrange, label=\"X\")\n",
    "\n",
    "for i in range(len(degres)):\n",
    "    poly = PolynomialFeatures(degree=degres[i], include_bias=False)\n",
    "    X_notes_train_poly = poly.fit_transform(X_notes_train)\n",
    "    reglogi = LogisticRegression(penalty='none')\n",
    "    reglogi.fit(X_notes_train_poly, Y_notes_train)\n",
    "    dessiner2D(reglogi, ax, xrange, yrange, poly=poly, colors=[couleurs[i+1]], label=\"X^\" + str(degres[i]))\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(\"Note 1\")\n",
    "plt.ylabel(\"Note 2\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b81abe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Good luck\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
